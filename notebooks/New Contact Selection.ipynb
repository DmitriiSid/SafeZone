{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data were loaded \n",
      "\n",
      "db_pomoci  (1456, 6)\n",
      "df_scraped_2_06  (3028, 5)\n",
      "maps_results  (1101, 11)\n",
      "Scraped data are ready \n",
      "\n",
      "DB data are ready \n",
      "\n",
      "Maps.cz data are ready \n",
      "\n",
      "Data baze obsahuje 828 schodnych kontaktu a 273 neschodnych kontaktu\n",
      "\n",
      "Time taken: 15.22 seconds\n",
      "Matched\n",
      "matched                              828\n",
      "new_email_match                      162\n",
      "new_contact_both_match_with_email     44\n",
      "unmatched                             25\n",
      "new_phone_match                       22\n",
      "new_contact_both_match                20\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Data baze obsahuje 828 schodnych kontaktu a 25 neschodnych kontaktu\n",
      "\n",
      "Time taken: 19.28 seconds\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from tqdm import tqdm \n",
    "import phonenumbers\n",
    "import phonenumbers.geocoder\n",
    "from urllib.parse import urljoin\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "\n",
    "RUN_EVIRONMENT = \"local\"\n",
    "\n",
    "if RUN_EVIRONMENT == \"local\":\n",
    "    DB_POMOCI_PATH =\"/Users/dmitriisid/Desktop/python/webscraping/db_pomoci.csv\"\n",
    "    #DB_POMOCI_PATH =\"/Users/dmitriisid/Desktop/python/webscraping/db_pomoci.csv\"\n",
    "    MAX_WORKERS = 4\n",
    "    SAVE_FILES_PATH = \"data/\"\n",
    "    MAPS_SCRAPED = \"/Users/dmitriisid/Desktop/python/webscraping/notebooks/wip/data/out.c_mapa_pomoci_output.maps_scraped.csv\"\n",
    "    #SCRAPED_DATA_PATH = '/Users/dmitriisid/Desktop/python/webscraping/data/2_06/df_scraped_2_06.csv'\n",
    "    SCRAPED_DATA_PATH = '/Users/dmitriisid/Desktop/python/webscraping/notebooks/wip/data/out.c_mapa_pomoci_output.df_scraped.csv'\n",
    "else:\n",
    "    raise EnvironmentError(\"this environment is not supported\")\n",
    "\n",
    "\n",
    "def has_more_than_3_consecutive_zeros(number):\n",
    "    return bool(re.search(r'0{4,}', str(number)))\n",
    "\n",
    "def py_parse_phonenumber(num):\n",
    "    try:\n",
    "        parsed_num = phonenumbers.parse(num, 'CZ')\n",
    "        phonenumbers.is_possible_number_with_reason(parsed_num)\n",
    "        return {\n",
    "            'formated_number':phonenumbers.format_number(parsed_num, phonenumbers.PhoneNumberFormat.E164),\n",
    "            'number': parsed_num.national_number,\n",
    "            'prefix': parsed_num.country_code,\n",
    "            'country_code': phonenumbers.region_code_for_number(parsed_num),\n",
    "            'valid': phonenumbers.is_valid_number(parsed_num),\n",
    "            'possible': phonenumbers.is_possible_number(parsed_num),\n",
    "            'parsed': True\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {'number': num, 'prefix': None, 'country_code': None, 'valid': False, 'possible': False, 'parsed': False}\n",
    "\n",
    "def udf(df: pd.DataFrame, column_name: str):\n",
    "    results = df[column_name].apply(py_parse_phonenumber)\n",
    "    parsed_df = pd.DataFrame(results.tolist())\n",
    "    return parsed_df\n",
    "\n",
    "def explode_df(df:pd.DataFrame, column_name:str, web_column = \"Base_Website\", scraped_web_column = \"Scraped_Page\") -> pd.DataFrame:\n",
    "    #df_res = pd.read_csv(\"../data/1_06/df_scraped_full.csv\")\n",
    "    df_res = df\n",
    "    df_res[f'{column_name}_scraped'] = df_res[column_name].str.split(', ')\n",
    "   # df_res['emails_scraped'] = df_res['Emails'].str.split(', ')\n",
    "    if scraped_web_column != \"Scraped_Page\":\n",
    "        df_res_phones = df_res[[web_column,f\"{column_name}_scraped\"]]\n",
    "        df_res_exp = df_res_phones.explode(f'{column_name}_scraped').reset_index(drop=True)\n",
    "        return df_res_exp\n",
    "    df_res_phones = df_res[[web_column, scraped_web_column,f\"{column_name}_scraped\"]]\n",
    "    df_res_exp = df_res_phones.explode(f'{column_name}_scraped').reset_index(drop=True)\n",
    "    return df_res_exp\n",
    "\n",
    "\n",
    "def clean_scraped_phones(df: pd.DataFrame, phone_scraped_column = \"Phone_Numbers\", web_column = \"Base_Website\",scraped_web_column = \"Scraped_Page\") -> pd.DataFrame:\n",
    "    phones_exp = explode_df(df,phone_scraped_column,web_column,scraped_web_column)\n",
    "    ress_df = udf(phones_exp,f\"{phone_scraped_column}_scraped\")\n",
    "    phones_exp[\"formated_number\"] = ress_df[\"formated_number\"]\n",
    "    phones_exp.drop(columns=[f\"{phone_scraped_column}_scraped\"], inplace= True)\n",
    "    phones_deduped = phones_exp.drop_duplicates(subset=[web_column, 'formated_number'])\n",
    "    filtered_df = phones_deduped[~phones_deduped['formated_number'].apply(has_more_than_3_consecutive_zeros)]\n",
    "    phones_df = filtered_df.sort_values(by=\"formated_number\", ascending=True)\n",
    "    return phones_df\n",
    "\n",
    "# Emails Part \n",
    "def clean_email(email, min_email_length = 4):\n",
    "    if pd.isna(email):\n",
    "        return email\n",
    "    \n",
    "    # Remove numbers at the beginning and leading/trailing spaces\n",
    "    email = email.strip()\n",
    "    # email = re.sub(r'^\\d+', '', email)\n",
    "    # email = re.sub(r',', '', email)\n",
    "    # email = re.sub(r' ', '', email)\n",
    "    # email = re.sub(r'\"', '', email)\n",
    "    email = re.sub(r'^[\\d]+', '', email)\n",
    "    email = re.sub(r'[,\\s\"]', '', email)\n",
    "    \n",
    "    # Regular expression to find valid email addresses\n",
    "    email_pattern = re.compile(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b')\n",
    "    \n",
    "    # Find all valid email addresses in the string and convert them to lowercase\n",
    "    valid_emails = [e.lower() for e in email_pattern.findall(email)]\n",
    "    \n",
    "    # Fix emails that have additional symbols after specified domains\n",
    "    domains = ['.cz', '.com', '.eu', '.org']\n",
    "    cleaned_emails = []\n",
    "    for email in valid_emails:\n",
    "        for domain in domains:\n",
    "            if email.endswith(domain):\n",
    "                email = re.sub(f'{domain}.*', domain, email)\n",
    "                break\n",
    "        # Exclude emails shorter than 3 symbols\n",
    "        if len(email) > min_email_length:\n",
    "            cleaned_emails.append(email)\n",
    "    \n",
    "    return ', '.join(cleaned_emails)\n",
    "    \n",
    "    return ', '.join(cleaned_emails)\n",
    "\n",
    "def clean_scraped_emails(df: pd.DataFrame, email_scraped_column = \"Emails\", web_column = \"Base_Website\") -> pd.DataFrame:\n",
    "    emails_exp = explode_df(df,email_scraped_column)\n",
    "    emails_exp[f'{email_scraped_column}_scraped'] = emails_exp[f'{email_scraped_column}_scraped'].apply(clean_email)\n",
    "    emails_deduped = emails_exp.drop_duplicates(subset=[web_column, f'{email_scraped_column}_scraped'])\n",
    "    # emails_sorted = emails_deduped.sort_values(by=\"Emails_scraped\", ascending=False)\n",
    "    return emails_deduped\n",
    "\n",
    "def db_pomoci_transform(df:pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function prepare data initial data for validation\n",
    "    \"\"\"\n",
    "    df['E_mail'] = df['E_mail'].apply(clean_email)\n",
    "    ress_df = udf(df,\"Telefon\")\n",
    "    df['Telefon'] = ress_df[\"formated_number\"]\n",
    "    df = df[~df[\"Webova_stranka\"].isna()]\n",
    "    df.loc[df['Webova_stranka'].str.startswith('www'), 'web'] = df['Webova_stranka'].str.replace('^www', 'https://www', regex=True)\n",
    "    return df\n",
    "  \n",
    "def main():\n",
    "    start_time = time.time()\n",
    "    db_pomoci = pd.read_csv(DB_POMOCI_PATH)\n",
    "    result_scraper = pd.read_csv(SCRAPED_DATA_PATH)\n",
    "    maps_results = pd.read_csv(MAPS_SCRAPED, sep=\",\")\n",
    "\n",
    "\n",
    "    print(\"Data were loaded \")\n",
    "    print(\"\")\n",
    "\n",
    "\n",
    "    print(\"db_pomoci \", db_pomoci.shape)\n",
    "    print(\"df_scraped_2_06 \", result_scraper.shape)\n",
    "    print(\"maps_results \", maps_results.shape)\n",
    "\n",
    "\n",
    "    df_phones_scraped = clean_scraped_phones(result_scraper)\n",
    "    df_emails_scraped = clean_scraped_emails(result_scraper)\n",
    "    df_emails_scraped = df_emails_scraped[['Base_Website', 'Scraped_Page', 'Emails_scraped']]\n",
    "    df_emails_scraped['Contact_type'] = 'Email'\n",
    "    df_emails_scraped.rename(columns={'Emails_scraped': 'Contact'}, inplace=True)\n",
    "\n",
    "    df_phones_scraped = df_phones_scraped[['Base_Website', 'Scraped_Page', 'formated_number']]\n",
    "    df_phones_scraped['Contact_type'] = 'Phone'\n",
    "    df_phones_scraped.rename(columns={'formated_number': 'Contact'}, inplace=True)\n",
    "\n",
    "    # Combine the DataFrames by appending rows\n",
    "    combined_df = pd.concat([df_phones_scraped, df_emails_scraped], ignore_index=True)\n",
    "\n",
    "    print(\"Scraped data are ready \")\n",
    "    print(\"\")\n",
    "\n",
    "    db_pomoci = db_pomoci_transform(db_pomoci)\n",
    "    print(\"DB data are ready \")\n",
    "    print(\"\")\n",
    "    maps_results = clean_scraped_phones(maps_results,phone_scraped_column = \"API_Phone\", web_column = \"Web\", scraped_web_column =  \"Web\")\n",
    "    #maps_results = clean_scraped_emails(maps_results, \"Email\")\n",
    "\n",
    "    maps_contacts = pd.concat([\n",
    "        #maps_results[['Email']].rename(columns={'Email': 'Contact'}),\n",
    "        maps_results[['Web','formated_number']].rename(columns={'formated_number': 'Contact'})\n",
    "    ]).dropna().drop_duplicates()\n",
    "\n",
    "    print(\"Maps.cz data are ready \")\n",
    "    print(\"\")\n",
    "\n",
    "    # Extract contacts from combined_df\n",
    "    scraped_contacts = combined_df[['Contact']].dropna().drop_duplicates()\n",
    "\n",
    "    def check_contact(contact, contacts_df, source_name):\n",
    "        \"\"\"\n",
    "        Check if a contact exists in the given contacts DataFrame.\n",
    "        \"\"\"\n",
    "        if contact in contacts_df['Contact'].values:\n",
    "            return source_name\n",
    "        return None\n",
    "\n",
    "    def match_contact(row, maps_contacts, scraped_contacts):\n",
    "        \"\"\"\n",
    "        Match contact details in the row with known contact sources.\n",
    "        \"\"\"\n",
    "        sources = set()\n",
    "\n",
    "        # Check against maps contacts\n",
    "        if check_contact(row['Telefon'], maps_contacts, 'maps_contacts'):\n",
    "            sources.add('maps_contacts_telefon')\n",
    "        # Check against scraped contacts\n",
    "        if check_contact(row['E_mail'], scraped_contacts, 'scraped_contacts'):\n",
    "            sources.add('scraped_contacts_email')\n",
    "        if check_contact(row['Telefon'], scraped_contacts, 'scraped_contacts'):\n",
    "            sources.add('scraped_contacts_telefon')        \n",
    "\n",
    "        if sources:\n",
    "            return 'matched', ', '.join(sources)\n",
    "        return 'unmatched', None\n",
    "\n",
    "    db_pomoci[['Matched', 'Source']] = db_pomoci.apply(\n",
    "        lambda row: pd.Series(match_contact(row, maps_contacts, scraped_contacts)), axis=1)\n",
    "\n",
    "    matched_num = db_pomoci[db_pomoci[\"Matched\"]==\"matched\"].shape[0]\n",
    "    unmatched_num = db_pomoci[db_pomoci[\"Matched\"]==\"unmatched\"].shape[0]\n",
    "    print(f\"Data baze obsahuje {matched_num} schodnych kontaktu a {unmatched_num} neschodnych kontaktu\")\n",
    "    print(\"\")\n",
    "\n",
    "    def find_new_contact(row):\n",
    "        if row['Matched'] == 'unmatched':\n",
    "            web = row['Webova_stranka']\n",
    "            maps_contact = maps_contacts[maps_contacts['Web'].str.contains(web, na=False) & ~(maps_contacts[\"Contact\"].isna())]\n",
    "            scraped_contact = combined_df[combined_df['Base_Website'].str.contains(web, na=False) & ~(combined_df[\"Contact\"].isna())]\n",
    "            common_contacts = pd.merge(maps_contact, scraped_contact, on='Contact')\n",
    "            scraped_email_contact = combined_df[(combined_df['Base_Website'].str.contains(web, na=False)) & (combined_df[\"Contact_type\"] == \"Email\") & ~(combined_df[\"Contact\"].isna())]\n",
    "            scraped_phone_contact = combined_df[(combined_df['Base_Website'].str.contains(web, na=False)) & (combined_df[\"Contact_type\"] == \"Phone\") & ~(combined_df[\"Contact\"].isna())]\n",
    "            new_contact = []\n",
    "            debug_info = []\n",
    "\n",
    "            if not common_contacts.empty and not scraped_email_contact.empty:\n",
    "                new_contact.extend(common_contacts['Contact'].values)\n",
    "                new_contact.extend(scraped_email_contact['Contact'].values)\n",
    "                debug_info.append(f\"Common contacts are {set(common_contacts['Contact'].values)}. There are also new emails {set(scraped_email_contact['Contact'].values)}\")\n",
    "                return pd.Series([new_contact, 'new_contact_both_match_with_email', ' | '.join(debug_info)])\n",
    "            \n",
    "            if not common_contacts.empty and scraped_email_contact.empty:\n",
    "                new_contact.extend(common_contacts['Contact'].values)\n",
    "                debug_info.append(f\"Common contacts are {set(common_contacts['Contact'].values)}, but there are not any new emails\")\n",
    "                return pd.Series([new_contact, 'new_contact_both_match', ' | '.join(debug_info)])\n",
    "            \n",
    "            if common_contacts.empty and not scraped_email_contact.empty:\n",
    "                new_contact.extend(scraped_email_contact['Contact'].values)\n",
    "                debug_info.append(\"Common contacts are empty, but there are new scraped emails\")\n",
    "                return pd.Series([new_contact, 'new_email_match', ' | '.join(debug_info)])\n",
    "            \n",
    "            if common_contacts.empty and not scraped_phone_contact.empty:\n",
    "                new_contact.extend(scraped_phone_contact['Contact'].values)\n",
    "                debug_info.append(\"Common contacts are empty, but there are new scraped phones\")\n",
    "                return pd.Series([new_contact, 'new_phone_match', ' | '.join(debug_info)])\n",
    "\n",
    "        return pd.Series([None, None, None])\n",
    "\n",
    "    find_new_contact_start = time.time()\n",
    "    \n",
    "    db_pomoci[['New Contact', 'New Matched', 'Explanation']] = db_pomoci.apply(find_new_contact, axis=1)\n",
    "\n",
    "    find_new_contact_end = time.time()\n",
    "    find_new_contact_time = find_new_contact_end - find_new_contact_start \n",
    "    print(f\"Time taken: {find_new_contact_time:.2f} seconds\")\n",
    "    # Update the contact information and flag accordingly\n",
    "    db_pomoci['Matched'] = db_pomoci.apply(\n",
    "        lambda row: row['New Matched'] if pd.notna(row['New Matched']) else row['Matched'], axis=1)\n",
    "    \n",
    "    db_pomoci['E_mail'] = db_pomoci.apply(\n",
    "        lambda row: next((contact for contact in row['New Contact'] if pd.isna(row['E_mail']) and pd.notna(contact)), row['E_mail'])\n",
    "        if row['New Contact'] is not None else row['E_mail'], axis=1)\n",
    "\n",
    "    db_pomoci['Telefon'] = db_pomoci.apply(\n",
    "        lambda row: next((contact for contact in row['New Contact'] if pd.isna(row['Telefon']) and pd.notna(contact)), row['Telefon'])\n",
    "        if row['New Contact'] is not None else row['Telefon'], axis=1)\n",
    "\n",
    "    # Drop the helper columns\n",
    "    \n",
    "    db_pomoci.drop(columns=[\"web\"], inplace= True)\n",
    "    db_pomoci.to_csv('wip/data/db_pomoci_flagged.csv', index=False)\n",
    "\n",
    "    matched_num = db_pomoci[db_pomoci[\"Matched\"]==\"matched\"].shape[0]\n",
    "    unmatched_num = db_pomoci[db_pomoci[\"Matched\"]==\"unmatched\"].shape[0]\n",
    "    print(db_pomoci[\"Matched\"].value_counts())\n",
    "    print(\"\")\n",
    "    print(f\"Data baze obsahuje {matched_num} schodnych kontaktu a {unmatched_num} neschodnych kontaktu\")\n",
    "    print(\"\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Time taken: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
