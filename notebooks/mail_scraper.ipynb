{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from tqdm import tqdm \n",
    "from urllib.parse import urljoin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep(subset:bool = False) -> pd.DataFrame:\n",
    "    db_pomoci = pd.read_csv(\"../db_pomoci.csv\")\n",
    "    df = db_pomoci.rename(columns={\"Webová stránka\" : \"web\", \"Název\":\"nazev\",\"E-mail\":\"email\"})\n",
    "    black_list = ['http://www.dc-brno.cz','www.freeklub.cz', 'http://www.cszs.cz/','http://www.fokustabor.cz/centrum-dusevniho-zdravi-_-komunitni-tym-tabor']\n",
    "    df = df[(~df[\"web\"].isna()) & (~df['web'].isin(black_list))]\n",
    "    df['web'] = df['web'].str.replace(r'\\s+', '', regex=True)\n",
    "    df.loc[df['web'].str.contains(r'\\.cz/.+'), 'web'] = df['web'].str.replace(r'\\.cz/.+', '.cz', regex=True)\n",
    "    df.loc[df['web'].str.startswith('www'), 'web'] = df['web'].str.replace('^www', 'https://www', regex=True)\n",
    "    df_agg_tel = df.groupby(\"web\")[\"Telefon\"].agg(list).reset_index()\n",
    "    df_agg_email = df.groupby(\"web\")[\"email\"].agg(list).reset_index()\n",
    "    df_agg = pd.merge(df_agg_tel, df_agg_email, on=\"web\")#.drop(\"index\", axis= 1)\n",
    "    df_agg = df_agg.rename(columns={'web': 'web', 'Telefon': 'Telefon'})\n",
    "    df_agg = pd.DataFrame(df_agg)\n",
    "    if subset:\n",
    "        df_agg = df_agg.head(300) # ubset first 300 rows\n",
    "    return df_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db_pomoci = pd.read_csv(\"db_pomoci.csv\")\n",
    "\n",
    "# black_list = ['http://www.dc-brno.cz','www.freeklub.cz, www.cszs.cz','https://www.freeklub.cz']\n",
    "\n",
    "# df = db_pomoci.rename(columns={\"Webová stránka\" : \"web\", \"Název\":\"nazev\",\"E-mail\":\"email\"})\n",
    "\n",
    "# df = df[(~df[\"web\"].isna()) & (~df['web'].isin(black_list))]\n",
    "\n",
    "# df.loc[df['web'].str.startswith('www'), 'web'] = df['web'].str.replace('^www', 'https://www', regex=True)\n",
    "\n",
    "# df_agg_tel = df.groupby(\"web\")[\"Telefon\"].agg(list).reset_index()\n",
    "# df_agg_email = df.groupby(\"web\")[\"email\"].agg(list).reset_index()\n",
    "# df_agg = pd.merge(df_agg_tel, df_agg_email, on=\"web\")#.drop(\"index\", axis= 1)\n",
    "# df_agg = df_agg.rename(columns={'web': 'web', 'Telefon': 'Telefon'})\n",
    "# df_agg = pd.DataFrame(df_agg)\n",
    "# df_agg = df_agg[~df_agg['web'].isin(black_list)]\n",
    "# df_agg.head()\n",
    "# #df.sort_values(\"web\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_agg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m matching_rows \u001b[38;5;241m=\u001b[39m \u001b[43mdf_agg\u001b[49m[df_agg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweb\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mcontains(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfreeklub\u001b[39m\u001b[38;5;124m'\u001b[39m, case\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, na\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Display the matching rows\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(matching_rows)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_agg' is not defined"
     ]
    }
   ],
   "source": [
    "matching_rows = df_agg[df_agg['web'].str.contains('freeklub', case=False, na=False)]\n",
    "\n",
    "# Display the matching rows\n",
    "print(matching_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_urls(df):    \n",
    "    email_regex = re.compile(r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\")\n",
    "    #Phone regex V2\n",
    "    phone_regex = re.compile(r\"\"\"\n",
    "        (?:\n",
    "            \\+420[-\\s]?        # Optional country code +420 followed by an optional space or dash\n",
    "        )?\n",
    "        (?:\n",
    "            \\d{3}[-\\s]?        # First part of the phone number (3 digits) followed by an optional space or dash\n",
    "            \\d{3}[-\\s]?        # Second part of the phone number (3 digits) followed by an optional space or dash\n",
    "            \\d{3}              # Third part of the phone number (3 digits)\n",
    "            |                  # OR\n",
    "            \\d{3}[-\\s]?        # First part of the phone number (3 digits) followed by an optional space or dash\n",
    "            \\d{2}[-\\s]?        # Second part of the phone number (2 digits) followed by an optional space or dash\n",
    "            \\d{2}[-\\s]?        # Third part of the phone number (2 digits) followed by an optional space or dash\n",
    "            \\d{2}              # Fourth part of the phone number (2 digits)\n",
    "        )\n",
    "    \"\"\", re.VERBOSE)\n",
    "    #Phone regex V1\n",
    "    # phone_regex = re.compile(r\"\"\"\n",
    "    #     (?:\n",
    "    #         \\+420[-\\s]?       # Optional country code +420 followed by an optional space or dash\n",
    "    #     )?\n",
    "    #     (?:                  # Non-capturing group for the phone number\n",
    "    #         \\d{3}[-\\s]?      # First part of the phone number (3 digits)\n",
    "    #         \\d{3}[-\\s]?      # Second part of the phone number (3 digits)\n",
    "    #         \\d{3}            # Third part of the phone number (3 digits)\n",
    "    #         (?:\\d{3})?       # Optional fourth part of the phone number (3 digits)\n",
    "    #     )\n",
    "    # \"\"\", re.VERBOSE)\n",
    "\n",
    "    # List to collect data\n",
    "    data = []\n",
    "\n",
    "    # Function to scrape a single URL\n",
    "    def scrape_website_main_page(url):\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()  # Raises an HTTPError if the response status code is 4XX or 5XX\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            emails = set(email_regex.findall(soup.text))\n",
    "            phones = set(phone_regex.findall(soup.text))\n",
    "            return emails, phones\n",
    "        except requests.RequestException as e:\n",
    "            #print(f\"Error fetching {url}: {e}\")\n",
    "            return set(),set()\n",
    "\n",
    "    # Function to scrape potential contact pages\n",
    "    def scrape_website_contacts(base_url, path='', visited_urls=set()):\n",
    "        full_url = urljoin(base_url, path)\n",
    "        if full_url in visited_urls:\n",
    "            return set()  # Return empty set if URL has already been visited\n",
    "        visited_urls.add(full_url)\n",
    "\n",
    "        try:\n",
    "            response = requests.get(full_url)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            emails = set(email_regex.findall(soup.text))\n",
    "            phones = set(phone_regex.findall(soup.text))\n",
    "\n",
    "            # Checking for links that may contain contact info\n",
    "            contact_links = []\n",
    "            for link in soup.find_all('a', href=True):\n",
    "                if 'kontakt' in link.text.lower() \\\n",
    "                    or 'contact' in link.text.lower() \\\n",
    "                    or 'kontakty' in link.text.lower() \\\n",
    "                    or 'kdo-jsem' in link.text.lower() \\\n",
    "                    or 'o-nas' in link.text.lower():\n",
    "                    contact_href = urljoin(full_url, link['href'])\n",
    "                    if contact_href not in visited_urls:\n",
    "                        contact_links.append(contact_href)\n",
    "\n",
    "            # Scrape each potential contact page\n",
    "            for link in contact_links:\n",
    "                sub_emails, sub_phones = scrape_website_main_page(link)\n",
    "                emails.update(sub_emails)\n",
    "                phones.update(sub_phones)\n",
    "\n",
    "            return emails, phones\n",
    "        except requests.RequestException as e:\n",
    "            #print(f\"Error fetching {full_url}: {e}\")\n",
    "            return set(),set()\n",
    "\n",
    "    # Set of URLs to scrape\n",
    "    urls = set(df[\"web\"])\n",
    "\n",
    "    # Loop through the URLs and scrape each one\n",
    "    visited_urls = set()\n",
    "    for url in tqdm(urls):\n",
    "        #print(f\"Scraping {url}\")\n",
    "        \n",
    "        emails, phones = scrape_website_contacts(url, visited_urls=visited_urls)\n",
    "        ###\n",
    "        # Condition if main page does not contain contact infromation\n",
    "        ###    \n",
    "        # if  ('' in emails) or  ('' in phones):        \n",
    "        #     emails_2, phones_2 = scrape_website_contacts(url, visited_urls=visited_urls)\n",
    "        #     emails.add(emails_2)\n",
    "        #     phones.add(phones_2)\n",
    "        emails_str = ', '.join(emails)\n",
    "        phones_str = ', '.join(phones)\n",
    "        \n",
    "        # Collect data\n",
    "        data.append({'Website': url, 'Emails': emails_str, 'Phone Numbers': phones_str})\n",
    "        #print(\"Done\")\n",
    "        #print(\"\")\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 3)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = data_prep(subset=True)\n",
    "#total_rows = df.shape[0]\n",
    "#df = df.loc[601:total_rows]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [08:55<00:00,  1.79s/it]\n"
     ]
    }
   ],
   "source": [
    "df_res = pd.DataFrame(scrape_urls(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res.to_csv('../data/29_05/df_scraped_v5_300.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Website</th>\n",
       "      <th>Emails</th>\n",
       "      <th>Phone Numbers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>http://www.psychologcb.cz/</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>http://www.cheiront.cz/</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>http://www.michaelvaclavik.cz/</td>\n",
       "      <td>NaN</td>\n",
       "      <td>+420 608 443 228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>http://www.cetera.cz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>http://www.kappa-help.cz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>+420 773 821 003, +420 773 821 001, +420 773 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>http://www.psycholog-tejklova.cz/</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>http://www.arcana.cz/</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>http://www.psychoterapeut-liberec.cz/</td>\n",
       "      <td>andrea.pirkova@gmail.com</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>http://psychologtabor.cz/</td>\n",
       "      <td>info@psychologtabor.cz</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>http://www.psychocentrumzlin.cz/</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>129 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Website                    Emails  \\\n",
       "7               http://www.psychologcb.cz/                       NaN   \n",
       "8                  http://www.cheiront.cz/                       NaN   \n",
       "12          http://www.michaelvaclavik.cz/                       NaN   \n",
       "13                    http://www.cetera.cz                       NaN   \n",
       "15                http://www.kappa-help.cz                       NaN   \n",
       "..                                     ...                       ...   \n",
       "287      http://www.psycholog-tejklova.cz/                       NaN   \n",
       "288                  http://www.arcana.cz/                       NaN   \n",
       "289  http://www.psychoterapeut-liberec.cz/  andrea.pirkova@gmail.com   \n",
       "290              http://psychologtabor.cz/    info@psychologtabor.cz   \n",
       "293       http://www.psychocentrumzlin.cz/                       NaN   \n",
       "\n",
       "                                         Phone Numbers  \n",
       "7                                                  NaN  \n",
       "8                                                  NaN  \n",
       "12                                    +420 608 443 228  \n",
       "13                                                 NaN  \n",
       "15   +420 773 821 003, +420 773 821 001, +420 773 0...  \n",
       "..                                                 ...  \n",
       "287                                                NaN  \n",
       "288                                                NaN  \n",
       "289                                                NaN  \n",
       "290                                                NaN  \n",
       "293                                                NaN  \n",
       "\n",
       "[129 rows x 3 columns]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scraped_v2_300_600 = pd.read_csv('data/29_05/df_scraped_v3_300.csv')\n",
    "df_scraped_v2_300_600[(df_scraped_v2_300_600['Emails'].isna()) | (df_scraped_v2_300_600['Phone Numbers'].isna())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5519630484988453"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = data_prep(subset=False)\n",
    "(df.shape[0]-(129 + 121 + 138))/df.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('data/29_05/df_scraped_v2_300.csv')\n",
    "df2 = pd.read_csv('data/29_05/df_scraped_v2_300_600.csv')\n",
    "df3 = pd.read_csv('data/29_05/df_scraped_v2_601_plus.csv')\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "concatenated_df = pd.concat([df1, df2, df3], ignore_index=True)\n",
    "\n",
    "# Save the concatenated DataFrame to a new CSV file\n",
    "concatenated_df.to_csv('df_scraped_v2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(866, 3)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched: cheiront@cheiront.cz\n"
     ]
    }
   ],
   "source": [
    "test_emails = ['cheiront@cheiront.cz']\n",
    "email_regex = re.compile(r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\")\n",
    "\n",
    "for email in test_emails:\n",
    "    match = email_regex.search(email)\n",
    "    if match:\n",
    "        print(f\"Matched: {match.group()}\")\n",
    "    else:\n",
    "        print(f\"No match: {email}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specific web check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_regex = re.compile(r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\")\n",
    "#Phone regex V2\n",
    "phone_regex = re.compile(r\"\"\"\n",
    "    (?:\n",
    "        \\+420[-\\s]?        # Optional country code +420 followed by an optional space or dash\n",
    "    )?\n",
    "    (?:\n",
    "        \\d{3}[-\\s]?        # First part of the phone number (3 digits) followed by an optional space or dash\n",
    "        \\d{3}[-\\s]?        # Second part of the phone number (3 digits) followed by an optional space or dash\n",
    "        \\d{3}              # Third part of the phone number (3 digits)\n",
    "        |                  # OR\n",
    "        \\d{3}[-\\s]?        # First part of the phone number (3 digits) followed by an optional space or dash\n",
    "        \\d{2}[-\\s]?        # Second part of the phone number (2 digits) followed by an optional space or dash\n",
    "        \\d{2}[-\\s]?        # Third part of the phone number (2 digits) followed by an optional space or dash\n",
    "        \\d{2}              # Fourth part of the phone number (2 digits)\n",
    "    )\n",
    "\"\"\", re.VERBOSE)\n",
    "#Phone regex V1\n",
    "# phone_regex = re.compile(r\"\"\"\n",
    "#     (?:\n",
    "#         \\+420[-\\s]?       # Optional country code +420 followed by an optional space or dash\n",
    "#     )?\n",
    "#     (?:                  # Non-capturing group for the phone number\n",
    "#         \\d{3}[-\\s]?      # First part of the phone number (3 digits)\n",
    "#         \\d{3}[-\\s]?      # Second part of the phone number (3 digits)\n",
    "#         \\d{3}            # Third part of the phone number (3 digits)\n",
    "#         (?:\\d{3})?       # Optional fourth part of the phone number (3 digits)\n",
    "#     )\n",
    "# \"\"\", re.VERBOSE)\n",
    "\n",
    "# List to collect data\n",
    "data = []\n",
    "\n",
    "# Function to scrape a single URL\n",
    "def scrape_website_main_page(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raises an HTTPError if the response status code is 4XX or 5XX\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        emails = set(email_regex.findall(soup.text))\n",
    "        phones = set(phone_regex.findall(soup.text))\n",
    "        return emails, phones\n",
    "    except requests.RequestException as e:\n",
    "        #print(f\"Error fetching {url}: {e}\")\n",
    "        return set(),set()\n",
    "\n",
    "# Function to scrape potential contact pages\n",
    "def scrape_website_contacts(base_url, path='', visited_urls=set()):\n",
    "    full_url = urljoin(base_url, path)\n",
    "    if full_url in visited_urls:\n",
    "        return set()  # Return empty set if URL has already been visited\n",
    "    visited_urls.add(full_url)\n",
    "\n",
    "    try:\n",
    "        response = requests.get(full_url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        emails = set(email_regex.findall(soup.text))\n",
    "        phones = set(phone_regex.findall(soup.text))\n",
    "\n",
    "        # Checking for links that may contain contact info\n",
    "        contact_links = []\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            if 'kontakt' in link.text.lower() \\\n",
    "                or 'contact' in link.text.lower() \\\n",
    "                or 'kontakty' in link.text.lower() \\\n",
    "                or 'kdo-jsem' in link.text.lower() \\\n",
    "                or 'o-nas' in link.text.lower():\n",
    "                contact_href = urljoin(full_url, link['href'])\n",
    "                if contact_href not in visited_urls:\n",
    "                    contact_links.append(contact_href)\n",
    "\n",
    "        # Scrape each potential contact page\n",
    "        for link in contact_links:\n",
    "            sub_emails, sub_phones = scrape_website_main_page(link)\n",
    "            emails.update(sub_emails)\n",
    "            phones.update(sub_phones)\n",
    "\n",
    "        return emails, phones\n",
    "    except requests.RequestException as e:\n",
    "        #print(f\"Error fetching {full_url}: {e}\")\n",
    "        return set(),set()\n",
    "web = 'http://www.psychoterapie-iz.cz/index.php/kontakt'\n",
    "print(scrape_website_main_page(web))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfr = pd.read_csv('../data/29_05/df_scraped_v3_300.csv')\n",
    "\n",
    "dfr[dfr['Phone Numbers'].isna()].shape\n",
    "# dfr[(dfr['Emails'].isna()) | (dfr['Phone Numbers'].isna())]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfr = pd.read_csv('../data/29_05/df_scraped_v5_300.csv')\n",
    "\n",
    "dfr[dfr['Phone Numbers'].isna()].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sublinks(base_url, path='', visited_urls=set()):\n",
    "    full_url = urljoin(base_url, path)\n",
    "    if full_url in visited_urls:\n",
    "        return set()  # Return empty set if URL has already been visited\n",
    "    visited_urls.add(full_url)\n",
    "    \n",
    "    print(f\"Visiting: {full_url}\")  # Debug statement\n",
    "\n",
    "    try:\n",
    "        response = requests.get(full_url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')        \n",
    "\n",
    "        # Checking for links that may contain contact info\n",
    "        contact_links = []\n",
    "        for link in soup.find_all('a', href=True):            \n",
    "            contact_href = urljoin(full_url, link['href'])\n",
    "            if contact_href not in visited_urls:\n",
    "                contact_links.append(contact_href)\n",
    "        \n",
    "        print(f\"Web {base_url} has {len(contact_links)} subpages\")  # Debug statement\n",
    "\n",
    "        sp = len(contact_links)\n",
    "        return sp\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error accessing {full_url}: {e}\")  # Debug statement\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = sublinks(url, visited_urls=visited_urls)\n",
    "sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = set(df[\"web\"])\n",
    "\n",
    "# Loop through the URLs and scrape each one\n",
    "visited_urls = set()\n",
    "total_pages = 0\n",
    "total_subpages = 0\n",
    "for url in urls:\n",
    "    total_pages +=1\n",
    "    print(f\"Total pages count: {total_pages}\")\n",
    "    \n",
    "    sp = sublinks(url, visited_urls=visited_urls)\n",
    "    total_subpages = total_subpages + sp\n",
    "    print(f\"Total subpages count: {total_subpages}\")\n",
    "    print(\"#####################\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "streamlit_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
